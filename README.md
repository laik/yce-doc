# 主题: 企业云原生实践

摘要:
在开发与运维人员的角度如何保证开发的敏捷应用程序的创建和部署的效率及可靠的硬件管理高可用成为了运营成本的重大投入; 在容器化管理之前,往往在应用开发与生产版本的运行时的不一致及移植业务割接工作都需要大量的开发运维工作量; 在容器化后解决了持续开发、集成和部署,通过快速简单的回滚(由于镜像不可变性)，提供可靠且频繁的容器镜像构建和部署; 并使得开发与运维的分离,在构建/发布时而不是在部署时创建应用程序容器镜像，从而将应用程序与基础架构分离; 可观察性不仅可以显示操作系统级别的信息和指标，还可以显示应用程序的运行状况和其他指标信号。对于容器管理,网络服务,资源管理等问题上基于kubernetes的架构二次开发云产品成为了企业的降本增效,快速支撑业务的首要目标; 在整体设计上,基于命名空间为环境,将用户分为管理用户,开发用户的不同操作视觉的维度; 对于运维用户更多的是做机器管理,命名空间资源配额(包括绑定node/绑定存储类/资源限定),存储管理等工作; 对于开发用户,更需要关注的是如何实现持续集成,快速上线,持续部署等工作;

## 1. Tekton 流水线的持续交付

### 背景
我们通常的开发流程是在本地开发完成应用之后，使用git或svn作为版本管理工具，将本地代码提交到类似版本管理仓库中做源代码持久化存储; 然而来自多个仓库涉及到多个中间件作为底层依赖一起部署到生产环境中时，大部份公司内部通常会有持继集成流程; 早期使用Jenkins工具链完成当下持续集成的工作流程,解决上层devops流程中的必要环节,但使用jenkins由master-slave的架构加之使用jvm运行及其内部使用DSL(groovy)拓展,对于服务器资源使用率,拓展性都不太理想;

### 提出问题
那么如何解决可以快速拓展及拥有强大调度能力框架及架构可以支持千量级并发持续集成的方案? 答案是肯定的，而且不乏竞争者，业内知名的有knavtieBuild/Jenkins/JenkinsX/Spinnaker/ArgoCD/Tekton，其中tekon凭借其众多优良特性在一众竞争者中胜出，成为领域内的事实标准, 我们公司在实践云原生的道路上的第一站,基于Tekton框架的高阶实现;

### 如何解决
...

### 其他(延伸点)
...

## 2. 自定义CRD增强部署管理

### 背景

当前我们在持续集成后的应用采用2种部署在物理机或者vm上(后统称node),docker化部署及传统裸机/vm(supervisor管理)部署在这些node上; 在运维人员的角度考虑如何做到高可用,负载均衡等基础资源架构; 需要借助大量的第三方工具链实现拓展; 其次这种组织在node架构上运行应用程序。对于运行node架构中的应用对于运维人员角度来看,比较难定义资源边界，这会导致资源分配问题。例如在node原本的基础上运行多个应用程序，则可能会出现一个应用程序占用大部分资源的情况，结果可能导致其他应用程序的性能下降。一种解决方案是在不同的node上运行每个应用程序，但是由于资源利用不足而无法扩展，管理的成本很高。

### 提出问题

* 问题1: 为了解决高可用问题,如何让应用部署可以感知地理位置感知并预先发布在每个不同机架的节点上防止机架掉电,机架网络交换机宕机等问题

* 问题2 如何解决更新应用的速度,大批量更新时的node的资源创建导致抖动

* 问题3 如何做到灵活的策略部署,并支持组的方式管理新旧版本实现蓝绿,金丝雀发布

### 如何解决

问题1 解决方案: 我们站在运维的角度思考,需要考虑真实机房机架安装服务器的供电,网络交换机的安装背景,还包括分布式存储的部署网络等情况等因素,解决应用部署至少需要实现不同机架的物理机冗余副本的高可用方式; 对此考虑,每个计算节点应当有地理信息,那么集群内的节点都需要运维人员进行添加与配置标识相应的区域,机架号,节点名称作为拓扑信息,应用发布时可以策略控制预先考虑在不同的机架机器上部署多副本,如图中对node的地理信息标识

![image](./chapter-2/node.png)

基于节点的地理信息标识,运维人员可以在创建命名空间绑定一批节点作为一个环境分配予开发用户(租户),此命名空间下发布的所有运行资源将会被调度在这批节点上
  
![image](./chapter-2/annotate-node.png)

对于划分环境的方式使用命名空间方式,有状态副本集使用存储也是属于运维人员的范畴,所以在命名空间的维度可以配置存储类

![image](./chapter-2/annotate-stge.png)

运维人员还可以根据申请配置命名空间进行精细的资源限定

![image](./chapter-2/resource-quota.png)

问题2 解决方案: kubernetes原生工作负载,不论是deployment、statefulset还是pod本身,如果你想升级pod中的镜像,那么kubernetes就会重新销毁该pod并重新调度并创建一个pod,对于statefulset虽然可以保持原有pod的名字,但是实际UID及pod的ip都将发生改变。如果你还使用了istio,那么在更新sidecar容器的时候,所有植入sidecar容器的pod都需要销毁、重新调度和重建,这将带来极大的开销,同时也影响了业务的稳定性。所以需要重新实现基于原生的控制器实现的一种资源控制器,可以在资源更新的时候实现原地升级(in-place update), 这种升级方式可以更新pod中某一个或多个容器的镜像版本,而不影响pod中其余容器的运行，同时保持pod的网络和存储状态不变。

问题3 解决方案: 资源控制器基于组的概念来解决发布时的应用的形态,我们内部实践设定了四种策略的方式:

* alpha: 挑选其中一组的一个发布1个pod,初次发布的应用尝试发布能否创建成功

* beta: 挑选(多组/一组)的一个发布1个pod,初次发布的应用尝试发布在每个组内(每个组可能在的机房/区域不同)能否创建成功

* omega: 挑选多组的每个机器上发布1个pod

* release: 全量发布

部署的布局如下图:

![image](./chapter-2/stone.png)

更新操作如下图:

![image](./chapter-2/stone2.png)


### 其他(延伸点)

对于资源控制器的要求将会在以后越来越多的业务场景,将会有更多新的基于kubernetes crd实现的控制器出现; 包括一些数据库应用的管理方式: 例如Mysql的集群架构,主从架构,或者Tidb,Mongodb等分布式的数据库,消息中间件等,后续需实现相应的专用的控制器;

## 3. 云原生下的运维管理(管理工具)


### 背景
通常我们一般在开发环境或者测试环境，遇到应用的网络不通，内存泄露，cpu飙升等，需要调试k8s里面的pod的业务容器，并且容器技术的一个最佳实践是构建尽可能精简的容器镜像。但这一实践却会给排查问题带来麻烦：精简后的容器中普遍缺失常用的排障工具，每次进去都需要（apt-get）去下载一些工具，给我们带来了极大的不便利性，甚至把基础镜像打的很大,导致在镜像不好传输等，为了让用户更方便的调试又精简容器的大小，我们需要在容器云推出一个很好的调试工具。

### 提出问题

那么针对上面的背景,我们需要这精简的容器并且又不需要安装其他命令的情况下，怎么去把这块的调试功能做好呢？

### 如何解决

在我们容器云平台里面，我们支持web-shell的功能,点击pod的时候，通过web-shell的方式进入容器里面,让用户针对它自己的业务容器排错，本身原理是非常简单的，如果理解了原理那么就能知道它是如何实现和如何使用的。

在讲这个运维工具之前，我们需要了解一下docker的基本原理，k8s是站在docker之上的，本质运行的还是docker，docker的本质是基于namespace隔离和cgroup资源限制，倘若，我们启动一个进程加入目标容器的namespace中，这样子


### 其他(延伸点)

...

## 4. 服务网格的实践

### 背景

...

### 提出问题

...

### 如何解决

...

### 其他(延伸点)

...
